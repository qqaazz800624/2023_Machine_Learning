{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **Homework 12 - Reinforcement Learning**\n",
        "\n",
        "If you have any problem, e-mail us at mlta-2023-spring@googlegroups.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXsnCWPtWSNk"
      },
      "source": [
        "## Preliminary work\n",
        "\n",
        "First, we need to install all necessary packages.\n",
        "One of them, gym, builded by OpenAI, is a toolkit for developing Reinforcement Learning algorithm. Other packages are for visualization in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e2bScpnkVbv",
        "outputId": "9898a461-e456-46b4-e05f-666ab92bfdd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "\u001b[33m\r0% [Connecting to security.ubuntu.com (185.125.190.39)] [Connecting to cloud.r-\u001b[0m\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.138.128.85)] [C\u001b[0m\r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "\u001b[33m\r                                                                               \r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)]\u001b[0m\r                                                                           \rHit:6 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-2build1).\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: box2d==2.3.2 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gym[box2d]==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.10/dist-packages (2.3.5)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: numpy==1.22.4 in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.25.2) (2.1.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.25.2) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: box2d==2.3.2 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: box2d-kengz in /usr/local/lib/python3.10/dist-packages (2.3.3)\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install python-opengl xvfb -y\n",
        "!pip install -q swig\n",
        "!pip install box2d==2.3.2 gym[box2d]==0.25.2 box2d-py pyvirtualdisplay tqdm numpy==1.22.4 \n",
        "!pip install box2d==2.3.2 box2d-kengz\n",
        "!pip freeze > requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_-i3cdoYsks"
      },
      "source": [
        "\n",
        "Next, set up virtual display，and import all necessaary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nl2nREINDLiw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "_exp = 'model1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaEJ8BUCpN9P"
      },
      "source": [
        "# Warning ! Do not revise random seed !!!\n",
        "# Your submission on JudgeBoi will not reproduce your result !!!\n",
        "Make your HW result to be reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fV9i8i2YkRbO"
      },
      "outputs": [],
      "source": [
        "seed = 2023 # Do not change this\n",
        "def fix(env, seed):\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0XDx6bzjgC"
      },
      "source": [
        "Last, call gym and build an [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N_4-xJcbBt09"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import gym\n",
        "import random\n",
        "env = gym.make('LunarLander-v2')\n",
        "fix(env, seed) # fix the environment Do not revise this !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkVvTrvWZ5H"
      },
      "source": [
        "## What Lunar Lander？\n",
        "\n",
        "“LunarLander-v2”is to simulate the situation when the craft lands on the surface of the moon.\n",
        "\n",
        "This task is to enable the craft to land \"safely\" at the pad between the two yellow flags.\n",
        "> Landing pad is always at coordinates (0,0).\n",
        "> Coordinates are the first two numbers in state vector.\n",
        "\n",
        "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
        "\n",
        "\"LunarLander-v2\" actually includes \"Agent\" and \"Environment\". \n",
        "\n",
        "In this homework, we will utilize the function `step()` to control the action of \"Agent\". \n",
        "\n",
        "Then `step()` will return the observation/state and reward given by the \"Environment\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbp82sljvAt"
      },
      "source": [
        "### Observation / State\n",
        "\n",
        "First, we can take a look at what an Observation / State looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rsXZra3N9R5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57653a7-5e0d-4a5e-f355-7f1c461f3ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
            " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
            " 1.       ], (8,), float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdfoThbAQ49"
      },
      "source": [
        "\n",
        "`Box(8,)`means that observation is an 8-dim vector\n",
        "### Action\n",
        "\n",
        "Actions can be taken by looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p1k4dIrBAaKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e441a052-43b8-4b2f-9978-fdb4aac66600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dejXT6PHBrPn"
      },
      "source": [
        "`Discrete(4)` implies that there are four kinds of actions can be taken by agent.\n",
        "- 0 implies the agent will not take any actions\n",
        "- 2 implies the agent will accelerate downward\n",
        "- 1, 3 implies the agent will accelerate left and right\n",
        "\n",
        "Next, we will try to make the agent interact with the environment. \n",
        "Before taking any actions, we recommend to call `reset()` function to reset the environment. Also, this function will return the initial state of the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pi4OmrmZgnWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f98670-110a-44ab-d912-8a967f60ab8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00506535  1.413064   -0.5130838   0.09527162  0.00587628  0.11622101\n",
            "  0.          0.        ]\n"
          ]
        }
      ],
      "source": [
        "initial_state = env.reset()\n",
        "print(initial_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBx0mEqqgxJ9"
      },
      "source": [
        "Then, we try to get a random action from the agent's action space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vxkOEXRKgizt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd033d7-dcc1-4b5d-c33a-a8e4ddc90227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "random_action = env.action_space.sample()\n",
        "print(random_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mns-bO01g0-J"
      },
      "source": [
        "More, we can utilize `step()` to make agent act according to the randomly-selected `random_action`.\n",
        "The `step()` function will return four values:\n",
        "- observation / state\n",
        "- reward\n",
        "- done (True/ False)\n",
        "- Other information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E_WViSxGgIk9"
      },
      "outputs": [],
      "source": [
        "observation, reward, done, info = env.step(random_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yK7r126kuCNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3bcb31-169c-47b0-bdb6-d0493a6a85a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdS8vOihxhc"
      },
      "source": [
        "### Reward\n",
        "\n",
        "\n",
        "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vxQNs77hi0_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c8d3ad-2e8e-48c5-c70a-34e79db3c998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.4981841929643156\n"
          ]
        }
      ],
      "source": [
        "print(reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhqp6D-XgHpe"
      },
      "source": [
        "### Random Agent\n",
        "In the end, before we start training, we can see whether a random agent can successfully land the moon or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5paWqo7tWL2"
      },
      "source": [
        "## DQN\n",
        "Now, we can build a Q network. The network will return one of action in the action space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "J8tdmeD-tZew"
      },
      "outputs": [],
      "source": [
        "#references: https://github.com/Singyuan/Machine-Learning-NTUEE-2022/blob/master/hw12/hw12.ipynb\n",
        "#references: https://github.com/yujunkuo/ML2022-Homework/blob/main/hw12/hw12_strong.ipynb\n",
        "#https://medium.com/pyladies-taiwan/reinforcement-learning-%E9%80%B2%E9%9A%8E%E7%AF%87-deep-q-learning-26b10935a745\n",
        "#https://ithelp.ithome.com.tw/articles/10208668\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size=8, action_size=4):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynbqJrhIFTC3"
      },
      "source": [
        "Then, we need to build a simple agent. The agent will acts according to the output of the policy network above. There are a few things can be done by agent:\n",
        "- `learn()`：update the policy network from log probabilities and rewards.\n",
        "- `sample()`：After receiving observation from the environment, utilize policy network to tell which action to take. The return values of this function includes action and log probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zZo-IxJx286z"
      },
      "outputs": [],
      "source": [
        "#references: https://github.com/Singyuan/Machine-Learning-NTUEE-2022/blob/master/hw12/hw12.ipynb\n",
        "#references: https://github.com/yujunkuo/ML2022-Homework/blob/main/hw12/hw12_strong.ipynb\n",
        "#https://medium.com/pyladies-taiwan/reinforcement-learning-%E9%80%B2%E9%9A%8E%E7%AF%87-deep-q-learning-26b10935a745\n",
        "#https://ithelp.ithome.com.tw/articles/10208668\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Parameters\n",
        "BUFFER_SIZE = int(1e5)\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate \n",
        "UPDATE_EVERY = 2        # how often to update the network\n",
        "CLIP_GRAD_NORM = 0.8\n",
        "\n",
        "class DQNAgent():\n",
        "\n",
        "    def __init__(self, state_size=8, action_size=4):\n",
        "        \n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
        "        self.t_step = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                idxs, is_weight, experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA, idxs, is_weight)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma, idxs, is_weights):      \n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "        loss = (torch.FloatTensor(is_weights) * F.mse_loss(Q_expected, Q_targets)).mean()\n",
        "        errors = torch.abs(Q_expected.detach() - Q_targets).data.squeeze().tolist()\n",
        "        self.memory.update(idxs, errors)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(), CLIP_GRAD_NORM)\n",
        "        self.optimizer.step()\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def get_mem_parms(self):\n",
        "        alpha, beta = self.memory.get_parm()\n",
        "        return alpha, beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iZTDiWOo6ikf"
      },
      "outputs": [],
      "source": [
        "#references: https://github.com/Singyuan/Machine-Learning-NTUEE-2022/blob/master/hw12/hw12.ipynb\n",
        "#references: https://github.com/yujunkuo/ML2022-Homework/blob/main/hw12/hw12_strong.ipynb\n",
        "#https://medium.com/pyladies-taiwan/reinforcement-learning-%E9%80%B2%E9%9A%8E%E7%AF%87-deep-q-learning-26b10935a745\n",
        "#https://ithelp.ithome.com.tw/articles/10208668\n",
        "\n",
        "\n",
        "# a binary tree data structure to represent that the parent’s value is the sum of its children\n",
        "class SumTree:\n",
        "    write = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object) \n",
        "        self.n_entries = 0\n",
        "\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "\n",
        "        self.tree[parent] += change\n",
        "\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(right, s - self.tree[left])\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    def add(self, p, data):\n",
        "        idx = self.write + self.capacity - 1\n",
        "\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "        if self.n_entries < self.capacity:\n",
        "            self.n_entries += 1\n",
        "\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    def get(self, s):\n",
        "        \n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "\n",
        "        return (idx, self.tree[idx], self.data[dataIdx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kONWhb5h6R05"
      },
      "outputs": [],
      "source": [
        "#references: https://github.com/Singyuan/Machine-Learning-NTUEE-2022/blob/master/hw12/hw12.ipynb\n",
        "#references: https://github.com/yujunkuo/ML2022-Homework/blob/main/hw12/hw12_strong.ipynb\n",
        "#https://medium.com/pyladies-taiwan/reinforcement-learning-%E9%80%B2%E9%9A%8E%E7%AF%87-deep-q-learning-26b10935a745\n",
        "#https://ithelp.ithome.com.tw/articles/10208668\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size):\n",
        "        \n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.tree =  SumTree(buffer_size)\n",
        "\n",
        "        self.buffer_size = buffer_size\n",
        "        self.prio_max = 0.1\n",
        "        self.alpha = 1.0\n",
        "        self.e = 0.01\n",
        "        self.beta = 0.5\n",
        "        self.beta_growth_rate = 1.005\n",
        "        self.alpha_decay_rate = 0.995\n",
        "\n",
        "        self.update_cnt = 0\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        data = self.experience(state, action, reward, next_state, done)\n",
        "        p = (np.abs(self.prio_max) + self.e) ** self.alpha \n",
        "        self.tree.add(p, data)\n",
        "    \n",
        "    def sample(self):\n",
        "        states_lst, actions_lst, rewards_lst, next_states_lst, dones_lst = [], [], [], [], []\n",
        "        idxs = []\n",
        "        segment = self.tree.total() / self.batch_size\n",
        "        priorities = []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            s = random.uniform(a, b)\n",
        "            idx, p, ex = self.tree.get(s)\n",
        "            if ex is not None:\n",
        "                states_lst.append(ex.state)\n",
        "                actions_lst.append(ex.action)\n",
        "                rewards_lst.append(ex.reward)\n",
        "                next_states_lst.append(ex.next_state)\n",
        "                dones_lst.append(ex.done)\n",
        "                priorities.append(p)\n",
        "                idxs.append(idx)\n",
        "                \n",
        "            \n",
        "        states = torch.from_numpy(np.vstack(states_lst)).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack(actions_lst)).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack(rewards_lst)).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack(next_states_lst)).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack(dones_lst).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        if self.update_cnt % 200 == 0:\n",
        "            self.beta = np.min([1., self.beta*self.beta_growth_rate])\n",
        "            self.alpha = np.max([0.5, self.alpha*self.alpha_decay_rate])\n",
        "        self.update_cnt += 1\n",
        "\n",
        "        sampling_probabilities = priorities / self.tree.total()\n",
        "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
        "        is_weight /= is_weight.max()\n",
        "\n",
        "        return idxs, is_weight, (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def update(self, idxs, errors):\n",
        "        self.prio_max = max(self.prio_max, max(np.abs(errors)))\n",
        "        for i, idx in enumerate(idxs):\n",
        "            p = (np.abs(errors[i]) + self.e) ** self.alpha\n",
        "            self.tree.update(idx, p) \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tree.n_entries\n",
        "\n",
        "    def get_parm(self):\n",
        "        return self.alpha, self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouv23glgf5Qt"
      },
      "source": [
        "## Training Agent\n",
        "\n",
        "Now let's start to train our agent.\n",
        "Through taking all the interactions between agent and environment as training data, the policy network can learn from all these attempts,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#references: https://github.com/Singyuan/Machine-Learning-NTUEE-2022/blob/master/hw12/hw12.ipynb\n",
        "#references: https://github.com/yujunkuo/ML2022-Homework/blob/main/hw12/hw12_strong.ipynb\n",
        "#https://medium.com/pyladies-taiwan/reinforcement-learning-%E9%80%B2%E9%9A%8E%E7%AF%87-deep-q-learning-26b10935a745\n",
        "#https://ithelp.ithome.com.tw/articles/10208668\n",
        "\n",
        "\n",
        "class DQN(object):\n",
        "  def __init__(self, env, n_episodes=3500, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    self.agent = DQNAgent()\n",
        "    self.env = env\n",
        "    self.n_episodes = n_episodes\n",
        "    self.max_t = max_t\n",
        "    self.eps_start = eps_start\n",
        "    self.eps_end = eps_end\n",
        "    self.eps_decay = eps_decay\n",
        "\n",
        "  def train(self):\n",
        "    scores, final_rewards = [], []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = self.eps_start                    \n",
        "    score_best = 0.0\n",
        "\n",
        "    for i_episode in range(1, self.n_episodes+1):\n",
        "      state = self.env.reset()\n",
        "      score = 0\n",
        "      for t in range(self.max_t):\n",
        "        action = self.agent.act(state, eps)\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        self.agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        score += reward\n",
        "        if done:\n",
        "            final_rewards.append(reward)\n",
        "            break \n",
        "      scores_window.append(score)       \n",
        "      scores.append(score)              \n",
        "      eps = max(self.eps_end, self.eps_decay*eps)\n",
        "\n",
        "      alpha, beta = self.agent.get_mem_parms()\n",
        "\n",
        "      if i_episode % 100 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tepsilon-greedy: {:.4f}\\talpha: {:.3f}, \\tbeta: {:.3f}'.format(i_episode, np.mean(scores_window), eps, alpha, beta))\n",
        "\n",
        "      if np.mean(scores_window)>=score_best+5:\n",
        "        print('Environment saved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        torch.save(self.agent.qnetwork_local.state_dict(), f'checkpoint_{_exp}.pth')\n",
        "        score_best = np.mean(scores_window).item()\n",
        "        if np.mean(scores_window)>=280.0:\n",
        "            print('Environment saved in {:d} episodes!\\tAverage Score: {:.2f} <-- exceed baseline'.format(i_episode, np.mean(scores_window)))\n",
        "            torch.save(self.agent.qnetwork_local.state_dict(), f'checkpoint_{_exp}.pth')\n",
        "            break\n",
        "    # final step\n",
        "    if np.mean(scores_window)>=score_best:\n",
        "      print('Environment saved in {:d} episodes!\\tAverage Score: {:.2f} <-- final step'.format(i_episode, np.mean(scores_window)))\n",
        "      torch.save(self.agent.qnetwork_local.state_dict(), f'checkpoint_{_exp}.pth')\n",
        "\n",
        "    return scores, final_rewards\n"
      ],
      "metadata": {
        "id": "CUFQufGE0g5q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DQN = DQN(env)\n",
        "scores, final_rewards = DQN.train()"
      ],
      "metadata": {
        "id": "e14M9oxvK-un",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b77a17-8e24-4046-e5fa-bddf3dd4cbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -180.35\tepsilon-greedy: 0.6058\talpha: 0.882, \tbeta: 0.566\n",
            "Episode 200\tAverage Score: -115.47\tepsilon-greedy: 0.3670\talpha: 0.711, \tbeta: 0.702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNb_tuFYhKVK"
      },
      "source": [
        "### Training Result\n",
        "During the training process, we recorded `avg_total_reward`, which represents the average total reward of episodes before updating the policy network.\n",
        "\n",
        "Theoretically, if the agent becomes better, the `avg_total_reward` will increase.\n",
        "The visualization of the training process is shown below:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZYOI8H10SHN"
      },
      "outputs": [],
      "source": [
        "plt.plot(scores)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV5jj4dThz0Y"
      },
      "source": [
        "In addition, `avg_final_reward` represents average final rewards of episodes. To be specific, final rewards is the last reward received in one episode, indicating whether the craft lands successfully or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txDZ5vlGWz5w"
      },
      "outputs": [],
      "source": [
        "plt.plot(final_rewards)\n",
        "plt.title(\"Final Rewards\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2HaGRVEYGQS"
      },
      "source": [
        "## Testing\n",
        "The testing result will be the average reward of 5 testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yFuUKKRYH73"
      },
      "outputs": [],
      "source": [
        "fix(env, seed)\n",
        "DQN.agent.qnetwork_local.load_state_dict(torch.load(f'checkpoint_{_exp}.pth'))\n",
        "#DQN.agent.qnetwork_local.load_state_dict(torch.load(f'checkpoint.pth'))\n",
        "# agent.network.eval()  # set the network into evaluation mode\n",
        "NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action = DQN.agent.act(state)\n",
        "      actions.append(action)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "\n",
        "      total_reward += reward\n",
        "      \n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) # save the result of testing \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aex7mcKr0J01"
      },
      "outputs": [],
      "source": [
        "print(np.mean(test_total_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leyebGYRpqsF"
      },
      "source": [
        "Action list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGAH4YWDpp4u"
      },
      "outputs": [],
      "source": [
        "print(\"Action list looks like \", action_list)\n",
        "print(\"Action list's shape looks like \", np.shape(action_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNkmwucrHMen"
      },
      "source": [
        "Analysis of actions taken by agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHdAItjj1nxw"
      },
      "outputs": [],
      "source": [
        "distribution = {}\n",
        "for actions in action_list:\n",
        "  for action in actions:\n",
        "    if action not in distribution.keys():\n",
        "      distribution[action] = 1\n",
        "    else:\n",
        "      distribution[action] += 1\n",
        "print(distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ricE0schY75M"
      },
      "source": [
        "Saving the result of Model Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZsMkGmIY42b"
      },
      "outputs": [],
      "source": [
        "PATH = \"d11948002_hw12.npy\" # Can be modified into the name or path you want\n",
        "np.save(PATH ,np.array(action_list)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asK7WfbkaLjt"
      },
      "source": [
        "### This is the file you need to submit !!!\n",
        "Download the testing result to your device\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-CqyhHzaWAL"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seT4NUmWmAZ1"
      },
      "source": [
        "# Server \n",
        "The code below simulate the environment on the judge server. Can be used for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U69c-YTxaw6b"
      },
      "outputs": [],
      "source": [
        "action_list = np.load(PATH,allow_pickle=True) # The action list you upload\n",
        "seed = 2023 # Do not revise this\n",
        "fix(env, seed)\n",
        "\n",
        "#agent.network.eval()  # set network to evaluation mode\n",
        "DQN.agent.qnetwork_local.load_state_dict(torch.load(f'checkpoint_{_exp}.pth'))\n",
        "#DQN.agent.qnetwork_local.load_state_dict(torch.load(f'checkpoint.pth'))\n",
        "#agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "\n",
        "test_total_reward = []\n",
        "if len(action_list) != 5:\n",
        "  print(\"Wrong format of file !!!\")\n",
        "  exit(0)\n",
        "for actions in action_list:\n",
        "  state = env.reset()\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  for action in actions:\n",
        "  \n",
        "      state, reward, done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  print(f\"Your reward is : %.2f\"%total_reward)\n",
        "  test_total_reward.append(total_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjFBWwQP1hVe"
      },
      "source": [
        "# Your score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpJpZz3Wbm0X"
      },
      "outputs": [],
      "source": [
        "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUBtYXG2eaqf"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Below are some useful tips for you to get high score.\n",
        "\n",
        "- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n",
        "- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n",
        "- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}